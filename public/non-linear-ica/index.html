<!DOCTYPE html>
<html lang="en-UK">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="http://localhost:1313/images/favicon2.png" />
<title>Non-linear ICA | =_O</title>
<meta name="title" content="Non-linear ICA" />
<meta name="description" content="An introduction to non-linear ICA." />
<meta name="keywords" content="" />


<meta property="og:url" content="http://localhost:1313/non-linear-ica/">
  <meta property="og:site_name" content="=_O">
  <meta property="og:title" content="Non-linear ICA">
  <meta property="og:description" content="An introduction to non-linear ICA.">
  <meta property="og:locale" content="en_UK">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2024-11-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-11-01T00:00:00+00:00">
    <meta property="og:image" content="http://localhost:1313/images/share.png">




  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/images/share.png">
  <meta name="twitter:title" content="Non-linear ICA">
  <meta name="twitter:description" content="An introduction to non-linear ICA.">




  <meta itemprop="name" content="Non-linear ICA">
  <meta itemprop="description" content="An introduction to non-linear ICA.">
  <meta itemprop="datePublished" content="2024-11-01T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-11-01T00:00:00+00:00">
  <meta itemprop="wordCount" content="420">
  <meta itemprop="image" content="http://localhost:1313/images/share.png">
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  body {
    font-family: Verdana, sans-serif;
    margin: auto;
    padding: 20px;
    max-width: 720px;
    text-align: left;
    background-color: #fff;
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: #444;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  strong,
  b {
    color: #222;
  }

  a {
    color: #4da7b4;
     
  }

  .title {
    text-decoration: none;
    border: 0;
  }

  .title span {
    font-weight: 400;
  }

  nav a {
    margin-right: 10px;
    color: #4da7b4;
  }

  textarea {
    width: 100%;
    font-size: 16px;
  }

  input {
    font-size: 16px;
  }

  content {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  img {
    max-width: 100%;
  }

  code {
    padding: 2px 5px;
    background-color: #f2f2f2;
  }

  pre code {
    color: #222;
    display: block;
    padding: 20px;
    white-space: pre-wrap;
    font-size: 14px;
    overflow-x: auto;
  }

  div.highlight pre {
    background-color: initial;
    color: initial;
  }

  div.highlight code {
    background-color: unset;
    color: unset;
  }

  blockquote {
    border-left: 1px solid #999;
    color: #222;
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px;
    text-align: center;
  }

  .helptext {
    color: #777;
    font-size: small;
  }

  .errorlist {
    color: #eba613;
    font-size: small;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
    align-items: bottom;
  }

  ul.blog-posts span {
    flex: 0 0 130px;
    padding-right: 14px;  
  }

  ul.blog-posts a:visited {
    color: #b46cca;
  }

  .date-code {
    font-family: monospace;
    font-size: 15px;
     
  }

  @media (prefers-color-scheme: dark) {
    body {
      background-color: #282c35;
      color: #fff;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    strong,
    b {
      color: #eee;
    }

    a {
      color: #4da7b4;
    }

    code {
      background-color: #777;
    }

    pre code {
      color: #ddd;
    }

    blockquote {
      color: #ccc;
    }

    textarea,
    input {
      background-color: #252525;
      color: #ddd;
    }

    .helptext {
      color: #aaa;
    }
  }

</style>

  <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script type="text/javascript">
      window.MathJax = {
          tex: {
              inlineMath: [['$', '$'], ['\\(', '\\)']],
              displayMath: [['$$', '$$'], ['\\[', '\\]']]
          },
          options: {
              skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      };
  </script>

  
</head>

<body>
  <header><a href="/" class="title">
  <h2>=_O</h2>
</a>
<nav><a href="/">\about</a>


<a href="/blog">\blog</a>

</nav>
</header>
  <main>

<h1>Non-linear ICA</h1>
<p>
  <span class="date-code">
    <time datetime='2024-11-01' pubdate>
      01 Nov, 2024
    </time>
  </span>
</p>

<content>
  <p>Tldr; Non-linear ICA replaces the mixing matrix with a non-linear mixing function.</p>
<hr>
<p>$\newcommand{\z}{\mathbf{z}}$
$\newcommand{\x}{\mathbf{x}}$
$\newcommand{\u}{\mathbf{u}}$
$\newcommand{\btheta}{\mathbf{\theta}}$
$\newcommand{\bphi}{\mathbf{\phi}}$
$\newcommand{\f}{\mathbf{f}}$
$\newcommand{\T}{\mathbf{T}}$
$\newcommand{\blambda}{\mathbf{\lambda}}$
$\newcommand{\bepsilon}{\mathbf{\varepsilon}}$</p>
<p>dimensions;: $\x\in\mathbb{R}^d$ and $\z\in\mathbb{R}^n$
deep latent variable model: $p_\btheta(\x,\z)=p_\btheta(\x|\z)p_\btheta(\z)$</p>
<p>decoder: $p_\theta(\x|\z)$</p>
<p>observed distribution: $p_\btheta(\x) = \int p_\btheta(\x,\z)d\z$</p>
<p>$\Rightarrow$ if $p_\btheta(\x|\z)$ is modelled with a neural network, we obtain a flexible class of distributions for $p_\btheta(\x)$</p>
<p>Variational autoencoders (VAEs) perform inference of the full generative model $p_\btheta(\x,\z)$ and the inference model $q_\bphi(\z|\x)$, which approximates $p_\btheta(\z|\x)$. However, it is difficult to obtain inference models that have meaning. The marginal distribution of $\x$ has meaning, but the distributions on the latent variables $\z$ are generally meaningless.</p>
<h1 id="identifiability">Identifiability</h1>
<p>Plan:</p>
<ul>
<li>Define identifiability</li>
<li>identifiability in linear ICA</li>
<li>intro to identifiability of non-linear ICA</li>
</ul>
<p>In general, identifiability in linear ICA is impossible without the assumptions in <a href="http://localhost:1313/infomax-and-linear-ica/">my previous post</a>.</p>
<h1 id="deep-latent-variable-models-with-conditional-latent-priors">Deep Latent Variable Models with Conditional Latent Priors</h1>
<p>Plan:</p>
<ul>
<li>Identifiability in non-linear ICA</li>
<li>Key assumptions + significance / meaning</li>
<li>Theory</li>
<li>Results</li>
<li>Conclusions</li>
</ul>
<p>Khemakhem et al. introduced the class of identifiable VAE models using a conditional prior on the latents $p_\btheta(\z|\u)$, where $\u \in \mathbb{R}^m$ is an observed random variable. By doing this, they essentially make the non-linear ICA problem into a semi-supervised signal disentanglement. In addition to this conditioning, they assume that the latent variables are distributed according to some exponential family of the form, $$p_{\T,\blambda} \prod_i \frac{Q_i(z_i)}{Z_i(\u)} \exp{\left[\sum_{j=1}^k T_{i,j}(z_i)\lambda_{i,j}(\u)\right]},$$ where $Q_i$ is the base measure, $Z_i(\u)$ is the normalising constant, \T_i=(T_{i,1}, \ldots, T_{i, k})$$ are the sufficient statistics, and $\blambda_i(\u)=(\lambda_{1,i}, \ldots, \lambda_{i,k})$. The parameters of the identifiable VAE are thus given by $\btheta=(\f, \T, \blambda)$.</p>
<p>The joint distribution of the $\x$ and $\z$ can be written as $$p_\btheta(\x,\z)p_{\T, \blambda}(\z|\u),$$ with, $$p_\f(\x|\z)=p_\bepsilon(\x - \f(\z)).$$ By taking, $\x - \f(\z) = \bepsilon \Rightarrow \x = \f(\z) + \bepsilon$, we obtain the non-linear ICA formulation, where $\f$ is a non-linear mixing function on $\z$ and $\bepsilon$ is an independent noise term. Additionally, assuming that $\f$ is injective, we guarantee that for each $\x$ there is a unique $\z$.</p>
<p><strong>Assumption.</strong> The function $\f: \mathbb{R}^d \rightarrow \mathbb{R}^m$ is injective.</p>
<p>The priors on the latent variables $p_\btheta(\z|\u)$ are assumed to be conditionally factorial with respect to $\u$. Thus, each individual latent $z_{i}$ is assigned a prior conditional on $\u$ through an arbitrary function $\blambda(\u)$, which outputs the parameters $\blambda_{i,j}$ exponential family parameters. This probability density is given by, $$p_{\T, \blambda}(\z|\u)=\prod_i \frac{Q_i(z_i)}{Z_i(\u)} \exp{\left[\sum_{j=1}^k T_{i,j}(z_i)\lambda_{i,j}(\u)\right]},$$ where $Q_i$ is the base measure, $Z_i(\u)$ is the normalising constant, \T_i=(T_{i,1}, \ldots, T_{i, k})$$ are the sufficient statistics, and $\blambda_i(\u)=(\lambda_{1,i}, \ldots, \lambda_{i,k})$.</p>
<h2 id="identifiability-of-dlvms-with-conditional-priors">Identifiability of DLVMs with Conditional Priors</h2>

</content>
<p>
  
</p>

  </main>
  <footer><a href="https://github.com/janraasch/hugo-bearblog/">ʕ•ᴥ•ʔ</a>
</footer>

    
</body>

</html>